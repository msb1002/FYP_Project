{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOR0nvFM9sd1bJO5tlu+ynO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msb1002/FYP_Project/blob/master/Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8_fvOzTytD_"
      },
      "source": [
        "italicized text# Models Pipeline\n",
        "\n",
        "Sun Bin MUN\n",
        "\n",
        "If the data are used in STOOQ, skip step 2 and \n",
        "go directly to Step 3 - model validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk-3B5ghyxvI"
      },
      "source": [
        "##1. Library Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmuB3k6mylAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef78ce4a-7c4c-4cdf-a714-c69c49f5edd7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import os.path as osp\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time, matplotlib, torch, os\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oksc1P41mEM"
      },
      "source": [
        "import os\n",
        "from os import walk\n",
        "\n",
        "import plotly\n",
        "%matplotlib inline\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from datetime import datetime\n",
        "#import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSYq86FJCYy3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BYyOxIKA8kd"
      },
      "source": [
        "#For DNN / CNN+LSTM\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, LSTM, Flatten, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeItdZ3TOgfa"
      },
      "source": [
        "#For SVR\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "#For SVM\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NK-tTTtr1lv"
      },
      "source": [
        "#For ARIMA\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import statsmodels as sm\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "#from pmdarima.arima import auto_arima\n",
        "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utUGtA77hMe3",
        "outputId": "5efd840a-630b-4f56-9b66-4bba2aa4f508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 33.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 4.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For CNN - LSTM\n",
        "import keras_tuner as kt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import InputLayer, ConvLSTM1D, LSTM, Flatten, RepeatVector, Dense, TimeDistributed\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "#from keras.losses import Huber"
      ],
      "metadata": {
        "id": "Lj5Bp6N4gjaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwrOy2XPy1Gr"
      },
      "source": [
        "##2. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "ADzBJ2jnBrzd",
        "outputId": "4c9ab927-a2b8-47e7-e65b-22fd1cff53b5"
      },
      "source": [
        "#@title\n",
        "DIR = '/content/drive/MyDrive/COMP_4981'\n",
        "\n",
        "#List of datasets\n",
        "dataset_folders = os.listdir(DIR)[1:]\n",
        "print(dataset_folders.sort())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-45a18631deea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#List of datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset_folders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_folders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/COMP_4981'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRQyIEt34P94"
      },
      "source": [
        "def finder(ds,target):\n",
        "  for e in ds:\n",
        "    if code in e:\n",
        "      return e\n",
        "\n",
        "def timestamp(i):\n",
        "  formatted_date = datetime.fromtimestamp(i)\n",
        "  return formatted_date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvHheiJnzqSm"
      },
      "source": [
        "code = input()+'.csv'\n",
        "filename = f'{DIR}/'+finder(dataset_folders,code)\n",
        "df = pd.read_csv(filename)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXRPkJux68cw"
      },
      "source": [
        "#Modify timestamp\n",
        "x = df['timestamp'].apply(lambda x: timestamp(x))\n",
        "df['date'] = x\n",
        "df.set_index(df['date'], inplace=True)\n",
        "df.drop(columns=['Unnamed: 0'],axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B-e_0Qm5Jx4"
      },
      "source": [
        "#Leave olhc\n",
        "df.set_index('date', inplace=True)\n",
        "df_olhcv = df[['Open','High','Low','Close','Volume']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00e0C9Z9JNlf"
      },
      "source": [
        "##2 b) Using pandas datareader for analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvyVNLBH5aFq"
      },
      "source": [
        "import pandas_datareader.data as web\n",
        "\n",
        "#Setting the index values as timestamp\n",
        "def process():\n",
        "  print(\"Enter the code of the model that you want to implement\")\n",
        "  code = input()\n",
        "  #final_code = '^'+code\n",
        "  final_code = code\n",
        "  df_ = web.DataReader(final_code, 'stooq')\n",
        "  #df_['timestamp'] = df_.index.astype('int64')\n",
        "  df_.reset_index(drop=True, inplace=True)\n",
        "  #df_.set_index('timestamp', inplace=True)\n",
        "\n",
        "  print(f\"Loading the dataset for the {code}\")\n",
        "\n",
        "  return df_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gliw90nI1X0"
      },
      "source": [
        "Reference : https://github.com/SaahilMadge-zz/Spring-2015-IW/blob/master/techsectoranalysis.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCFhGI_3LbWa"
      },
      "source": [
        "def additional_features(df):\n",
        "  #SMA\n",
        "  df['10_sma'] = df['Close'].rolling(window=10,min_periods=1).mean()\n",
        "  df['20_sma'] = df['Close'].rolling(window=20,min_periods=1).mean()\n",
        "  df['50_sma'] = df['Close'].rolling(window=50,min_periods=1).mean()\n",
        "  \n",
        "  #EMA\n",
        "  df['10_ema'] = df['Close'].ewm(span=10,min_periods=1).mean()\n",
        "  df['20_ema'] = df['Close'].ewm(span=20,min_periods=1).mean()\n",
        "  df['50_ema'] = df['Close'].ewm(span=50,min_periods=1).mean()\n",
        "\n",
        "  #Bollinger\n",
        "  df['bollinger_mean'] = df['Close'].rolling(20, min_periods=1).mean()\n",
        "  df['bollinger_std'] = df['Close'].rolling(20, min_periods=1).std()\n",
        "  df['BOL_UP'] = df['bollinger_mean'] + (2 * df['bollinger_std'])\n",
        "  df['BOL_DOWN'] = df['bollinger_mean'] - (2 * df['bollinger_std'])\n",
        "  df[\"bollinger_gap\"] = df[\"BOL_UP\"]-df['BOL_DOWN']\n",
        "  df.dropna(inplace=True) #Not sure if I can change?\n",
        "\n",
        "  price_change =  df['Close'].pct_change().fillna(0)\n",
        "  df['percetage_change'] = price_change\n",
        "\n",
        "  momentum = [1,1]\n",
        "  for i in range(2,len(df)):\n",
        "    momentum.append(1 if df['Close'][i] > df['Close'][i-1] else -1)\n",
        "  df['momentum'] = momentum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "-u2K68y1JSX8",
        "outputId": "a201c8a1-984b-40c3-d41e-8612806afb3c"
      },
      "source": [
        "#Print the database for the implementations\n",
        "\n",
        "df = process()\n",
        "additional_features(df)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the code of the model that you want to implement\n",
            "AAPL\n",
            "Loading the dataset for the AAPL\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-655bf031-63ec-41b4-b3e4-3c8e63b9eea5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>10_sma</th>\n",
              "      <th>20_sma</th>\n",
              "      <th>50_sma</th>\n",
              "      <th>10_ema</th>\n",
              "      <th>20_ema</th>\n",
              "      <th>50_ema</th>\n",
              "      <th>bollinger_mean</th>\n",
              "      <th>bollinger_std</th>\n",
              "      <th>BOL_UP</th>\n",
              "      <th>BOL_DOWN</th>\n",
              "      <th>bollinger_gap</th>\n",
              "      <th>percetage_change</th>\n",
              "      <th>momentum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>170.00</td>\n",
              "      <td>171.08</td>\n",
              "      <td>165.940</td>\n",
              "      <td>166.23</td>\n",
              "      <td>94814990</td>\n",
              "      <td>165.370000</td>\n",
              "      <td>165.370000</td>\n",
              "      <td>165.370000</td>\n",
              "      <td>165.456000</td>\n",
              "      <td>165.413000</td>\n",
              "      <td>165.387200</td>\n",
              "      <td>165.370000</td>\n",
              "      <td>1.216224</td>\n",
              "      <td>167.802447</td>\n",
              "      <td>162.937553</td>\n",
              "      <td>4.864895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>171.51</td>\n",
              "      <td>172.54</td>\n",
              "      <td>169.405</td>\n",
              "      <td>169.80</td>\n",
              "      <td>91168729</td>\n",
              "      <td>166.846667</td>\n",
              "      <td>166.846667</td>\n",
              "      <td>166.846667</td>\n",
              "      <td>167.202259</td>\n",
              "      <td>167.023880</td>\n",
              "      <td>166.917355</td>\n",
              "      <td>166.846667</td>\n",
              "      <td>2.698376</td>\n",
              "      <td>172.243419</td>\n",
              "      <td>161.449915</td>\n",
              "      <td>10.793504</td>\n",
              "      <td>0.021476</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>171.34</td>\n",
              "      <td>173.78</td>\n",
              "      <td>171.090</td>\n",
              "      <td>173.07</td>\n",
              "      <td>80440780</td>\n",
              "      <td>168.402500</td>\n",
              "      <td>168.402500</td>\n",
              "      <td>168.402500</td>\n",
              "      <td>169.135418</td>\n",
              "      <td>168.769301</td>\n",
              "      <td>168.549011</td>\n",
              "      <td>168.402500</td>\n",
              "      <td>3.812693</td>\n",
              "      <td>176.027885</td>\n",
              "      <td>160.777115</td>\n",
              "      <td>15.250770</td>\n",
              "      <td>0.019258</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>175.78</td>\n",
              "      <td>176.62</td>\n",
              "      <td>171.790</td>\n",
              "      <td>172.19</td>\n",
              "      <td>84505760</td>\n",
              "      <td>169.160000</td>\n",
              "      <td>169.160000</td>\n",
              "      <td>169.160000</td>\n",
              "      <td>170.012306</td>\n",
              "      <td>169.596739</td>\n",
              "      <td>169.336606</td>\n",
              "      <td>169.160000</td>\n",
              "      <td>3.710997</td>\n",
              "      <td>176.581994</td>\n",
              "      <td>161.738006</td>\n",
              "      <td>14.843989</td>\n",
              "      <td>-0.005085</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>176.12</td>\n",
              "      <td>177.18</td>\n",
              "      <td>174.820</td>\n",
              "      <td>175.53</td>\n",
              "      <td>74805173</td>\n",
              "      <td>170.221667</td>\n",
              "      <td>170.221667</td>\n",
              "      <td>170.221667</td>\n",
              "      <td>171.445441</td>\n",
              "      <td>170.848386</td>\n",
              "      <td>170.474756</td>\n",
              "      <td>170.221667</td>\n",
              "      <td>4.216636</td>\n",
              "      <td>178.654938</td>\n",
              "      <td>161.788395</td>\n",
              "      <td>16.866543</td>\n",
              "      <td>0.019397</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-655bf031-63ec-41b4-b3e4-3c8e63b9eea5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-655bf031-63ec-41b4-b3e4-3c8e63b9eea5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-655bf031-63ec-41b4-b3e4-3c8e63b9eea5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Open    High      Low  ...  bollinger_gap  percetage_change  momentum\n",
              "1  170.00  171.08  165.940  ...       4.864895          0.000000         1\n",
              "2  171.51  172.54  169.405  ...      10.793504          0.021476         1\n",
              "3  171.34  173.78  171.090  ...      15.250770          0.019258         1\n",
              "4  175.78  176.62  171.790  ...      14.843989         -0.005085         1\n",
              "5  176.12  177.18  174.820  ...      16.866543          0.019397        -1\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEuORRvCJC_J"
      },
      "source": [
        "## 3. Model Selections (in function)\n",
        "\n",
        "For demonstration, all the models are from STOOQ for simplicity. Implement Step 1,2 for latter versions\n",
        "\n",
        "Further references - https://pandas-datareader.readthedocs.io/en/latest/remote_data.html#remote-data-stooq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmGbpfw97uWB"
      },
      "source": [
        "## 3 a) SVR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd5Bw0sk7tpN"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "def svr_func(df_):\n",
        "  train_df = df_\n",
        "  #Data Preprocessing\n",
        "  train_df_svr = train_df.head(len(train_df)-1) \n",
        "  days = train_df_svr.index.tolist()\n",
        "  days = [[i] for i in days]\n",
        "  #adj_prices = [[i] for i in adj_prices]\n",
        "  adj_prices = train_df_svr['Close'].tolist()\n",
        "  train_X, test_X, train_y, test_y = train_test_split(days, adj_prices, test_size=0.25)\n",
        "\n",
        "  #SVR Models-1\n",
        "  #lin_svr = SVR(kernel='linear',C=1000.0)\n",
        "  #lin_svr.fit(train_X,train_y)\n",
        "\n",
        "  #SVR Models-2\n",
        "  poly_svr = SVR(kernel='poly',C=1000.0, degree=2)\n",
        "  poly_svr.fit(train_X,train_y)\n",
        "\n",
        "  #SVR Models-3\n",
        "  rbf_svr = SVR(kernel='rbf',C=1000.0, gamma = 0.15)\n",
        "  rbf_svr.fit(train_X,train_y)\n",
        "\n",
        "  #performance evaluation\n",
        "\n",
        "  print(\"Pure prediction section\")\n",
        "  print(\"Actual score - poly\")\n",
        "  print(poly_svr.score(test_X, test_y))\n",
        "  print(\"Actual score - rbf\")\n",
        "  print(rbf_svr.score(test_X, test_y))\n",
        "\n",
        "  print(\"------------------------------------\")\n",
        "  #svr_list = [lin_svr,poly_svr,rbf_svr]\n",
        "  svr_list = [poly_svr,rbf_svr]\n",
        "  for svr in svr_list:\n",
        "    svr_predict = svr.predict(test_X)\n",
        "\n",
        "    print('SVR {} PERFORMANCE'.format(str(svr)))\n",
        "    print('r2 score: '+str(r2_score(test_y, svr_predict)))\n",
        "    print('RMSE : '+str(np.sqrt(mean_squared_error(test_y, svr_predict))))\n",
        "    print(\"Mean Absolute Error : \" + str(mean_absolute_error(test_y,svr_predict)))\n",
        "\n",
        "\n",
        "def svm_func(df_):\n",
        "\n",
        "  print(\"Momentum prediction - as classifier\")\n",
        "\n",
        "  length = len(df)\n",
        "\n",
        "  X = np.transpose(np.array([df['percetage_change'],df['bollinger_gap'],df['bollinger_mean'],df['bollinger_std']]))\n",
        "  Y = np.array(df['momentum'])\n",
        "\n",
        "  X_train = X[0:int(0.8*length)]\n",
        "  X_test = X[int(0.8*length):]\n",
        "  y_train = Y[0:int(0.8*length)]\n",
        "  y_test = Y[int(0.8*length):]\n",
        "\n",
        "\n",
        "  poly_svm = svm.SVC(kernel='poly')\n",
        "  poly_svm.fit(X_train, y_train)\n",
        "  poly_score = poly_svm.score(X_test, y_test)\n",
        "  print(\"Prediction score for poly SVM classifier : \",poly_score)\n",
        "\n",
        "  rbf_svm = svm.SVC(kernel='rbf')\n",
        "  rbf_svm.fit(X_train, y_train)\n",
        "  rbf_score = rbf_svm.score(X_test, y_test)\n",
        "  print(\"Prediction score for RBF SVM classifier : \",rbf_score)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = process()\n",
        "additional_features(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-b827bRCMy-",
        "outputId": "0108a590-dcb8-4bd9-9194-ae6a6d616527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the code of the model that you want to implement\n",
            "AAPL\n",
            "Loading the dataset for the AAPL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY2HHB6lS5Q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb2f044-cea2-488a-f742-bed8ad823d9a"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import svm\n",
        "\n",
        "svr_func(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pure prediction section\n",
            "Actual score - poly\n",
            "0.6003790985255301\n",
            "Actual score - rbf\n",
            "0.9912817020330122\n",
            "------------------------------------\n",
            "SVR SVR(C=1000.0, degree=2, kernel='poly') PERFORMANCE\n",
            "r2 score: 0.6003790985255301\n",
            "RMSE : 26.84928045252851\n",
            "Mean Absolute Error : 21.38156694624261\n",
            "SVR SVR(C=1000.0, gamma=0.15) PERFORMANCE\n",
            "r2 score: 0.9912817020330122\n",
            "RMSE : 3.965741551375899\n",
            "Mean Absolute Error : 1.8467377734498047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0wJDjr9eFNF",
        "outputId": "301d3033-d936-413d-8e83-ba70e035c71e"
      },
      "source": [
        "svm_func(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Momentum prediction - as classifier\n",
            "Prediction score for poly SVM classifier :  0.5476190476190477\n",
            "Prediction score for RBF SVM classifier :  0.5476190476190477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "sxj24hAIeaI1",
        "outputId": "8127a980-36eb-4979-e790-b273b0625f84"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 1002)\n",
        "\n",
        "tuned_parameters = [{'kernel': ['linear', 'poly', 'rbf'],'C': [1,100,1000]}]\n",
        "\n",
        "clf = GridSearchCV(SVC(), tuned_parameters, scoring='accuracy')\n",
        "clf.fit(train_X, train_y)\n",
        "\n",
        "clf.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8740d01569d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtuned_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'poly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rbf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA1ApSg5eh-U"
      },
      "source": [
        "clf.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XiLZ4JVfEHL"
      },
      "source": [
        "Reference : https://stackoverflow.com/questions/62346013/svc-object-has-no-attribute-svc\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiOVimFRe39d"
      },
      "source": [
        "means = clf.cv_results_['mean_test_score']\n",
        "stds = clf.cv_results_['std_test_score']\n",
        "clf.cv_results_['params']\n",
        "\n",
        "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "          % (mean, std * 2, params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dhk5wk28guF"
      },
      "source": [
        "## 3 b) DNN Model (pure)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_KXWpZD8YRG"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_k4NZBk8hEU"
      },
      "source": [
        "#Data Pre-processing\n",
        "train_df = df\n",
        "\n",
        "X = train_df[['Open','High','Low','Volume']]\n",
        "Y = train_df[['Close']]\n",
        "\n",
        "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X[X.columns] = X_scaler.fit_transform(X[X.columns])\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "Y[Y.columns] = Y_scaler.fit_transform(Y[Y.columns])\n",
        "Y = Y.fillna(Y.median())\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, Y, test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89MD-JWwPuo7"
      },
      "source": [
        "#Model\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Dense(64, activation=\"relu\", input_shape=(4,)),\n",
        "        layers.Dense(128, activation=\"relu\", name=\"layer1\"),\n",
        "        layers.Dense(256, activation=\"relu\", name=\"layer2\"),\n",
        "        layers.Dense(1,activation=\"linear\", name=\"layer3\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(loss='mean_squared_error',optimizer='adam')\n",
        "history = model.fit(train_X,train_y,validation_data=(val_X, val_y),epochs = 200 ,batch_size = 10,verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "history_df['loss'].plot();\n",
        "history_df['val_loss'].plot();\n",
        "plt.title(\"Simple DNN Architecture\")"
      ],
      "metadata": {
        "id": "-GAFWwmEb4Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvRNawsbP45r"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76EX1r2NP8_q"
      },
      "source": [
        "dnn_predict = model.predict(test_X)\n",
        "\n",
        "print('SIMPLE DNN PERFORMANCE')\n",
        "print('r2 score: '+str(r2_score(test_y, dnn_predict)))\n",
        "print('RMSE : '+str(np.sqrt(mean_squared_error(test_y, dnn_predict))))\n",
        "print(\"Mean Absolute Error : \" + str(mean_absolute_error(test_y,dnn_predict)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 c) Complex DNN Model"
      ],
      "metadata": {
        "id": "HpyiPwr77B_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_y, test_y = train_test_split(X, Y, test_size=0.25)\n",
        "train_X,val_X,train_y,val_y = train_test_split(train_X, train_y, test_size=0.25)"
      ],
      "metadata": {
        "id": "ec7U-2U0HrMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    min_delta=0.0001, # minimium change considered improvement\n",
        "    patience=50, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "#train_X, test_X, train_y, test_y\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(units=1024, input_shape=[len(train_X.columns)]),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1)\n",
        "\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_X, train_y,\n",
        "    validation_data=(val_X, val_y),\n",
        "    batch_size=256,\n",
        "    epochs=200,\n",
        "    callbacks=[early_stopping], # put your callbacks in a list\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "HFpDWcg27BsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Expressing the loss - underfitting\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df['loss'].plot();\n",
        "history_df['val_loss'].plot();\n",
        "plt.title(\"Complex DNN Structure\")"
      ],
      "metadata": {
        "id": "yBpO_-_a7BJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4tgv8On-1v9"
      },
      "source": [
        "## 3 c) XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-3YeEha-4qt"
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, Y, test_size=0.2)\n",
        "model = XGBRegressor(learning_rate = 0.01,n_estimators=1000)\n",
        "\n",
        "model.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(test_X, test_y)], verbose=False)\n",
        "xgb_pred_y = model.predict(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUkJl2OyQFvU"
      },
      "source": [
        "#Performance evaluation\n",
        "print('XGBOOST PERFORMANCE')\n",
        "print('r2 score: '+str(r2_score(test_y, xgb_pred_y)))\n",
        "print('RMSE : '+str(np.sqrt(mean_squared_error(test_y, xgb_pred_y))))\n",
        "print(\"Mean Absolute Error : \" + str(mean_absolute_error(test_y,xgb_pred_y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmGjDj1U-5LW"
      },
      "source": [
        "## 3 d) CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o86lI_xw-65I"
      },
      "source": [
        "pip install catboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qiGwOy0QXDX"
      },
      "source": [
        "from catboost import CatBoostRegressor\n",
        "\n",
        "model_cat = CatBoostRegressor(random_state=1002,logging_level='Silent')\n",
        "\n",
        "model_cat.fit(train_X, train_y, plot=True)\n",
        "pred_cat = model_cat.predict(test_X, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6DTbG9XQctx"
      },
      "source": [
        "#Performance evaluation\n",
        "print('CATBoost PERFORMANCE')\n",
        "print('r2 score: '+str(r2_score(test_y, pred_cat)))\n",
        "print('RMSE : '+str(np.sqrt(mean_squared_error(test_y, pred_cat))))\n",
        "print(\"Mean Absolute Error : \" + str(mean_absolute_error(test_y,pred_cat)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLGtEzfzf9E9"
      },
      "source": [
        "## 3 e) ARIMA\n",
        "\n",
        "Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBIhfNBL_5ew"
      },
      "source": [
        "ARIMA - Auto Regression Integrated Moving Average (AR, I, MA)\n",
        "\n",
        "**p**: order of the autoregressive model (how lagged it is) -> # of preceding Y values that needs to be added\n",
        "\n",
        "**d**: degree of differencing (How much the value is different from the previous) -> d until it makes data stationary (const variance)\n",
        "\n",
        "e.g. \n",
        "\n",
        "If d=1:  yt  =  Yt - Yt-1\n",
        "\n",
        "If d=2:  yt  =  (Yt - Yt-1) - (Yt-1 - Yt-2)  = Yt - 2Yt-1 + Yt-2\n",
        "\n",
        "**q** : Order of moving average (MA) - error terms (adding the noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lvXmDlLBV4a"
      },
      "source": [
        "Criteria \n",
        "\n",
        "- ACF,PACF graph : appropriateness of the model\n",
        "- AIC : Lowest\n",
        "- BIC : Lowest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvoG9w7kQqYR"
      },
      "source": [
        "df_close = df['Close']\n",
        "\n",
        "#For the ADF test\n",
        "\n",
        "def test_stationarity(timeseries):\n",
        "    #Determing rolling statistics\n",
        "    rolmean = timeseries.rolling(12).mean() #window size 12 mean\n",
        "\n",
        "    rolstd = timeseries.rolling(12).std() #windoiw size 12 std\n",
        "    #Plot rolling statistics:\n",
        "    plt.plot(timeseries, color='blue',label='Base')\n",
        "    plt.plot(rolmean, color='red', label='Rolling Mean')\n",
        "    plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Rolling Mean and Standard Deviation')\n",
        "    plt.show(block=False)\n",
        "\n",
        "    print(\"Results of dickey fuller test\")\n",
        "    adft = adfuller(timeseries,autolag='AIC')\n",
        "\n",
        "    # Resulting test statistics, p-value, lags used and number of observation\n",
        "    output = pd.Series(adft[0:4],index=['Test Statistics','p-value','# lags','# observations used'])\n",
        "    for key,values in adft[4].items():\n",
        "        output['critical value (%s)'%key] =  values\n",
        "    print(output)\n",
        "    \n",
        "test_stationarity(df_close)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TVoPJM_ELva"
      },
      "source": [
        "Unit Root Test : \n",
        "Time Series y_t = D_t + z_t + e_t\n",
        "- D_t : Deterministic (trend,season)\n",
        "- z_t : stochastic (random walk with a drift) **testing this part**\n",
        "- e_t : stationary error\n",
        "\n",
        "Model with unit root has a \"spike and shocks\" = high unit root means highly stochastic\n",
        "\n",
        "\n",
        "Critical value (1%,5%,10% C.I) < Test Statistics\n",
        "\n",
        "H_0 : (O) unit root exists vs H_a : (X) unit root does not exist\n",
        "\n",
        "NOT Stationary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf6KqIQEsPzy"
      },
      "source": [
        "#The data is non-linear\n",
        "#Analyzing the Close, Trend, and seasonality\n",
        "result = seasonal_decompose(df_close, model='multiplicative', period = 30)\n",
        "\n",
        "fig = plt.figure()  \n",
        "fig = result.plot()  \n",
        "fig.set_size_inches(16, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rAYOw_-DYEG"
      },
      "source": [
        "Based on the graph above, it is evident that there is a downward trend in Close price, meaning that the data is **not stationary**\n",
        "\n",
        "- mean should be stationary over time\n",
        "- variance should not be function of time (homoscedasticity)\n",
        "-covariance should not be function of time\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4FzezL3HgX9"
      },
      "source": [
        "Methods to make data stationary : https://people.duke.edu/~rnau/whatuse.htm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoG9qPMzFINA"
      },
      "source": [
        "#Log,Differencing to make it stationary - Transformations such as logarithms can help to stabilise the variance of a time series. \n",
        "df_log = np.log(df_close)\n",
        "moving_avg = df_log.rolling(12).mean()\n",
        "std_dev = df_log.rolling(12).std()\n",
        "\n",
        "#Separating the training and testing\n",
        "train_data, test_data = df_log[:int(len(df_log)*0.9)], df_log[int(len(df_log)*0.9):]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvuWHDWdJwIp"
      },
      "source": [
        "Autocorrelation : relationship between the lags (serial correlation)\n",
        "- X axis : # of lags\n",
        "- Y axis : Moran's I (how one is similar to surrounding)\n",
        "\n",
        "Below\n",
        "\n",
        "- ACF : Quite close to one another and is all in one side\n",
        "-PACF : Part of y(t) vs y(t-2) not explained by y(t-1)\n",
        "\n",
        "AR : ACF decrease slowly & PACF sharp drop\n",
        "\n",
        "MA : ACF sharp drop & PACF gradual decline\n",
        "\n",
        "ARMA : moth ACF & PACF decrease\n",
        "\n",
        "e.g. 둘 다 서서히 줄어들면 ARMA(1,1), ACF에서 두번 쭉 떨어지면 MA(2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5EPdRj-JMMW"
      },
      "source": [
        "#Auto-correlation\n",
        "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
        "plot_acf(Y**2)\n",
        "plt.show()\n",
        "\n",
        "#Partial Auto-correlation\n",
        "plot_pacf(Y**2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg2dYwF6INfJ"
      },
      "source": [
        "#Selecting the mest ARIMA combination\n",
        "model_autoARIMA = auto_arima(train_data, start_p=0, start_q=0,\n",
        "                      test='adf',       # use adftest to find optimal 'd'\n",
        "                      max_p=10, max_q=10, # maximum p and q\n",
        "                      m=1,              # frequency of series\n",
        "                      d=None,           # let model determine 'd'\n",
        "                      seasonal=False,   # No Seasonality\n",
        "                      start_P=0, \n",
        "                      D=0, \n",
        "                      trace=True,\n",
        "                      error_action='ignore',  \n",
        "                      suppress_warnings=True, \n",
        "                      stepwise=True)\n",
        "\n",
        "print(model_autoARIMA.summary())\n",
        "\n",
        "model_autoARIMA.plot_diagnostics(figsize=(15,8))\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2VA53QCOHT4"
      },
      "source": [
        "Reference: https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.plot_diagnostics.html\n",
        "\n",
        "Produces a 2x2 plot grid with the following plots (ordered clockwise from top left):\n",
        "\n",
        "1. Standardized residuals over time\n",
        "\n",
        "2. Histogram plus estimated density of standardized residuals, along with a Normal(0,1) density plotted for reference. (KDE - Epanechnikov kernel function)\n",
        "\n",
        "3. Normal Q-Q plot, with Normal reference line. (finding the apt distribution)\n",
        "\n",
        "4. Correlogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLRVq_EdMtZ_"
      },
      "source": [
        "#As expected, better to use the AR > MA focused model\n",
        "#Best model:  ARIMA(9,1,0)(0,0,0)[0]\n",
        "\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "model = ARIMA(train_data, order=(9,1,0))  \n",
        "arima_model = model.fit()\n",
        "\n",
        "print(arima_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XpGoEzoSzjd"
      },
      "source": [
        "#train,test -> after the log / difference\n",
        "plt.plot(train_data,color='blue')\n",
        "\n",
        "model = ARIMA(train_data[1:], order=(9,1,0))\n",
        "results = model.fit()\n",
        "plt.plot(results.fittedvalues[1:], color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvnQ-k3gXv3Y"
      },
      "source": [
        "model = ARIMA(train_data, order=(9,1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJiNSysNas6R"
      },
      "source": [
        "r = model.fit()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPs8jZyIay40"
      },
      "source": [
        "x = r.predict(start=len(train_data),end=len(df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAoQsBKHWQ8Q"
      },
      "source": [
        "plt.plot(train_data,color='blue')\n",
        "plt.plot(results.fittedvalues[1:], color='red')\n",
        "plt.plot(test_data,color='green')\n",
        "plt.plot(x,color='orange')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN7Tfm6CH79-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvaG6n8CH8VK"
      },
      "source": [
        "Autoregressive models are used for \n",
        "1. Stationary (AR)\n",
        "2. Trend (ARIMA)\n",
        "3. Seasonality (SARIMA)\n",
        "\n",
        "\n",
        "ARCH - AR(p) model applied to variance of time series\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MK8WpTGcfEe"
      },
      "source": [
        "## 3 f) RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIqr0SUjcevQ"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "final_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n",
        "                              ('model', RandomForestRegressor(n_estimators=50,random_state=0))\n",
        "                             ])\n",
        "# cv returns negative MAE, so *-1\n",
        "scores = -1 * cross_val_score(final_pipeline, X, Y,\n",
        "                              cv=5, #how many partitions they have\n",
        "                              scoring='neg_mean_absolute_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "id": "7riGiDI8feH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxAvhIsxhca6"
      },
      "source": [
        "## 3 e) CNN + LSTM\n",
        "\n",
        "Reference : https://machinelearningmastery.com/cnn-long-short-term-memory-networks/\n",
        "\n",
        "### define CNN model\n",
        "cnn = Sequential()\n",
        "\n",
        "cnn.add(Conv2D(...))\n",
        "\n",
        "cnn.add(MaxPooling2D(...))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "### define LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(TimeDistributed(cnn, ...))\n",
        "\n",
        "model.add(LSTM(..))\n",
        "\n",
        "model.add(Dense(...))"
      ]
    }
  ]
}